---
title: "Caret"
output: html_notebook
---


```{r}
library(magrittr)
# visualizations ----------------------------------------------------------

str(iris)

library(AppliedPredictiveModeling)
transparentTheme(trans = 0.4)
library(caret)
featurePlot(x = iris[, 1:4], y = iris$Species, plot = 'pairs', auto.key = list(columns = 3))


```
```{r}
featurePlot(x = iris[, 1:4], y = iris$Species, plot = 'ellipse', auto.key = list(columns = 3))
```
Overlayed Density Plots
```{r}
transparentTheme(trans = 0.9)
featurePlot(
  x = iris[,1:4],
  y = iris$Species,
  plot = 'density',
  scales = list(
    x = list(relation = 'free'),
    y = list(relation = 'free')
  ),
  adjust = 1.5,
  pch = '|',
  layout = c(4,1),
  auto.key = list(columns = 3)
)

```

```{r}
featurePlot(
  x = iris[, 1:4],
  y = iris$Species,
  plot = 'box',
  scales = list(
    y = list(relation = 'free'),
    x = list(rot = 90)
  ),
  layout = c(4, 1),
  auto.key = list(columns = 2)
)
```

Scatter plots
For regression, the Boston Housing data is used:
```{r}
library(mlbench)
data("BostonHousing")
regVar <- c('age', 'lstat', 'tax')
str(BostonHousing[, regVar])



```

```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(0.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col <- rgb(1,0,0,.7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
featurePlot(
  x = BostonHousing[, regVar],
  y = BostonHousing$medv,
  plot = 'scatter',
  layout = c(3, 1)
)
```

```{r}
featurePlot(
  x = BostonHousing[, regVar],
  y = BostonHousing$medv,
  plot = 'scatter',
  type = c('p', 'smooth'),
  span = .5,
  layout = c(3, 1)
)
```



```{r}
library(earth)
data(etitanic)
head(etitanic)
head(model.matrix(survived ~ ., data = etitanic))

dummies <- dummyVars(survived ~ ., data = etitanic)
head(dummies)
head(predict(dummies, newdata = etitanic))
```


```{r}
data(mdrr)
data.frame(table(mdrrDescr$nR11))


```


```{r}
nzv <- nearZeroVar(mdrrDescr, saveMetrics = TRUE)
head(nzv)
nzv[nzv$nzv, ][1:10,]
```

```{r}
dim(mdrrDescr)
```
```{r}
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)
```

```{r}
data("GermanCredit")
library(MASS)
r <- lda(formula = Class ~ ., data = GermanCredit)

x <- nearZeroVar(GermanCredit, saveMetrics = TRUE)

x[x[, 'zeroVar'], ]

x[x[, 'zeroVar'] + x[, 'nzv'] > 0, ] %>% dim()

```

Identifying Correlated Predictors

```{r}
descrCor <- cor(filteredDescr)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) >.999)

summary(descrCor[upper.tri(descrCor)])

highlyCorDescr <-  findCorrelation(descrCor, cutoff = 0.75)

filteredDescr <- filteredDescr[, -highlyCorDescr]
descrCor2 <- cor(filteredDescr)
summary(descrCor2[upper.tri(descrCor2)])


```


```{r}
ltfrDesign <- matrix(0, nrow = 6, ncol = 6)
ltfrDesign[,1] <- c(1, 1, 1, 1, 1, 1)
ltfrDesign[,2] <- c(1, 1, 1, 0, 0, 0)
ltfrDesign[,3] <- c(0, 0, 0, 1, 1, 1)
ltfrDesign[,4] <- c(1, 0, 0, 1, 0, 0)
ltfrDesign[,5] <- c(0, 1, 0, 0, 1, 0)
ltfrDesign[,6] <- c(0, 0, 1, 0, 0, 1)

comboInfo <- findLinearCombos(ltfrDesign)

ltfrDesign[, -comboInfo$remove]

```

The `preProcess` Function
Centering and Scaling

Understanding scale in R.

Scale with default settings will calculate the mean and standard deviation for the entire vector, the 'scale' each element by those values by subtracting the mean and dividig by the sd.

if scale = FALSE, iit will only subtract the mean but not divide by the std deviation.

It provides nothing else but a standardization of the data. The values it creates are known under several different names, one of them being z-score

Fisher z-transformation in statistics.


```{r}
set.seed(96)
inTrain <- sample(seq(along = mdrrClass), length(mdrrClass) / 2)

training <- filteredDescr[inTrain, ]
test <- filteredDescr[-inTrain, ]
preProcValues <- preProcess(training, method = c('center', 'scale'))



```






