---
title: "MLR basics"
output: html_notebook
---

```{r}
library(mlr)
library(magrittr)
data(BostonHousing, package = 'mlbench')
regr.task <- makeRegrTask(id = 'bh', data = BostonHousing, target = 'medv')
regr.task

data(BreastCancer, package = 'mlbench')
df <- BreastCancer
head(df)
df$Id <- NULL
classif.task <- makeClassifTask(id = 'BreastCancer', data = df, target = 'Class', positive = 'malignant')


removeConstantFeatures(classif.task)
dropFeatures(task = classif.task, features = c('Cell.size'))
task <- normalizeFeatures(obj = classif.task, method = 'range')
summary(getTaskData(task))

```
Learner
```{r}
classif.lrn <- makeLearner(cl = 'classif.randomForest', predict.type = 'prob', fix.factors.prediction = TRUE)
classif.lrn$par.set
classif.lrn$par.vals
names(classif.lrn)
getHyperPars(learner = classif.lrn)
getParamSet(classif.lrn)
getLearnerParamSet(classif.lrn)
getLearnerParVals(classif.lrn)

getParamSet('classif.randomForest')
getLearnerShortName(learner = classif.lrn)

classif.lrn <- setPredictType(learner = classif.lrn, predict.type = 'response')
lrns <- listLearners()
names(lrns)
head(lrns[c('class', 'package')])
lrns <- listLearners(obj = classif.task, properties = 'prob', check.packages = F)

```
Train
```{r}
task <- makeClassifTask(id = 'iris', data = iris, target = 'Species')
lrn <- makeLearner(cl = 'classif.lda')

mod <- train(learner = lrn, task = task)

mod <- train('surv.coxph', lung.task)
data(ruspini, package = 'cluster')

plot(y~x, ruspini)


ruspini.task <- makeClusterTask(data = ruspini)
listLearners(obj = ruspini.task, warn.missing.packages = F, check.packages = F)
lrn <- makeLearner('cluster.kmeans', centers = 4)
mod <- train(learner = lrn, task = ruspini.task)
names(mod)
mod$learner
mod$learner.model
mod$task.desc
mod$features
getLearnerModel(mod)

n <- getTaskSize(bh.task)
train.set <- sample(n, size = n / 3)
mod = train(learner = 'regr.lm', bh.task, subset = train.set)



target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
w <- 1/tab[target]
train('classif.rpart', task = bc.task, weights = w) -> mod
```
Prediction
```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(2, n, by = 2)
lrn <- makeLearner(cl = 'regr.gbm', n.trees = 100)
mod <- train(learner = lrn, task = bh.task, subset = train.set)
task.pred <- predict(mod, task = bh.task, subset = test.set)


n <- nrow(iris)
iris.train <- iris[seq(1, n, by = 2), -5]
iris.test <- iris[seq(2, n, by = 2), -5]
task <- makeClusterTask(data = iris.train)
mod <- train('cluster.kmeans', task)


newdata.pred <- predict(mod, newdata = iris.test)


lrn.lm = makeLearner("regr.lm", predict.type = 'se')
mod.lm = train(lrn.lm, bh.task, subset = train.set)
task.pred.lm = predict(mod.lm, task = bh.task, subset = test.set)
task.pred.lm



lrn = makeLearner("cluster.cmeans", predict.type = "prob")
mod = train(lrn, mtcars.task)

pred = predict(mod, task = mtcars.task)
head(getPredictionProbabilities(pred))


mod = train("classif.lda", task = iris.task)

pred = predict(mod, task = iris.task)
pred

lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)

pred = predict(mod, newdata = iris)
head(getPredictionProbabilities(pred))
calculateConfusionMatrix(pred)
conf.matrix <- calculateConfusionMatrix(pred = pred, relative = T, sums = T)



lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)

### Label of the positive class
getTaskDesc(sonar.task)$positive

pred1 = predict(mod, sonar.task)
pred1$threshold
calculateConfusionMatrix(pred = pred1, relative = T, sums = T)

pred2 = setThreshold(pred1, 0.9)
pred2
calculateConfusionMatrix(pred = pred2, relative = T, sums = T)
```
Visualizing the prediction
```{r}
lrn = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)
```
Data preprocessing
```{r}
sonar.task
lrn <- makePreprocWrapperCaret(learner = 'classif.qda', ppc.pca = T, ppc.thresh = 0.9)
mod <- train(lrn, sonar.task)

rin <- makeResampleInstance("CV", iters = 3, stratify = T, task = sonar.task)
res <- benchmark(list('classif.qda', lrn), sonar.task, rin, show.info = FALSE)


getParamSet(lrn)

ps <- makeParamSet(
  makeIntegerParam('ppc.pcaComp', lower = 1, upper = getTaskNFeats(sonar.task)),
  makeDiscreteParam('predict.method', values = c('plug-in', 'debiased'))
)

ctrl = makeTuneControlGrid(resolution = 10)
res = tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE)
res
```
Performance


```{r}
n = getTaskSize(bh.task)
lrn = makeLearner("regr.gbm", n.trees = 1000)
mod = train(lrn, task = bh.task, subset = seq(1, n, 2))
pred = predict(mod, task = bh.task, subset = seq(2, n, 2))

performance(pred, measures = list(mse, medse, mae, timetrain, timepredict), model = mod)

lrn = makeLearner("cluster.kmeans", centers = 3)
mod = train(lrn, mtcars.task)
pred = predict(mod, task = mtcars.task)

# Calculate the Dunn index
performance(pred, measures = dunn, task = mtcars.task)

lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)
pred = predict(mod, task = sonar.task)

performance(pred, measures = list(auc, mmce, fpr))


lrn = makeLearner("classif.lda", predict.type = "prob")
n = getTaskSize(sonar.task)
mod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))
pred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))

# Performance for the default threshold 0.5
performance(pred, measures = list(fpr, fnr, mmce))
d = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
plotThreshVsPerfGGVIS(d)


r = calculateROCMeasures(pred)
```

Resampling

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample('regr.lm', bh.task, rdesc)

r

rdesc = makeResampleDesc("Subsample", iters = 5)

# Subsampling with 5 iterations and 4/5 training data
rdesc = makeResampleDesc("Subsample", iters = 50, split = 4/5)

# Classification tree with information splitting criterion
lrn = makeLearner("classif.rpart", parms = list(split = "information"))

# Calculate the performance measures
r = resample(lrn, sonar.task, rdesc, measures = list(mmce, fpr, fnr, timetrain))
r
```
Tuning hyperparameters

```{r}
# create search space
ps <- makeParamSet(
  makeNumericParam("C", lower = 0.01, up = 0.1)
)

# optimization algorithm 
ctrl <- makeTuneControlRandom(maxit = 100L)

# evalutate method use 3-fold CV
rdesc <- makeResampleDesc(method = 'CV', iters = 3L)
measure <- acc


# 1. search space and optimization
# 2. how to visualize the hyperparameter tuning effects

discrete_ps <- makeParamSet(
  makeDiscreteParam(id = "C", values = c(0.5, 1.0, 1.5, 2.0)),
  makeDiscreteParam(id = 'sigma', values = c(0.5, 1.0, 1.5, 2.0))
)
discrete_ps

num_ps <- makeParamSet(
  makeNumericParam('C', lower = -10, upper = 10, trafo = function(x) 10^x),
  makeNumericParam('sigma', lower = -10, upper = 10, trafo = function(x) 10^x)
)

ctrl <- makeTuneControlGrid(resolution = 15L)
ctrl <- makeTuneControlRandom(maxit = 10L)

rdesc <- makeResampleDesc(method = 'CV', iters = 3L)


# use the SVM parameters

ctrl <- makeTuneControlGrid()

res_dis <- tuneParams(learner = 'classif.ksvm', task = iris.task, resampling = rdesc, measures = acc, par.set = discrete_ps, control = ctrl)

ctrl <- makeTuneControlRandom(maxit = 100L)

res_num <- tuneParams(learner = 'classif.ksvm', task = iris.task, resampling = rdesc, measures = list(acc, setAggregation(acc, test.sd)), par.set = num_ps, control = ctrl, show.info = FALSE)

lrn <- setHyperPars(makeLearner('classif.ksvm'), par.vals = res_num$x)
m <- train(learner = lrn, task = iris.task)

pred <- predict(m, task = iris.task)

performance(pred = pred, measures = acc, task = iris.task, model = m)
calculateConfusionMatrix(pred = pred)
pred

generateHyperParsEffectData(res_num)

rdesc2 <- makeResampleDesc(method = 'Holdout', predict = 'both')
res2 <- tuneParams(learner = 'classif.ksvm', task = iris.task, resampling = rdesc2, measures = list(acc, setAggregation(acc, train.mean)), par.set = num_ps, control = ctrl, show.info = FALSE)
generateHyperParsEffectData(res2)


res <- tuneParams('classif.ksvm', task = iris.task, resampling = rdesc, par.set = num_ps, control = ctrl, measures = list(acc, mmce), show.info = FALSE)
data <- generateHyperParsEffectData(res)
plotHyperParsEffect(hyperpars.effect.data = data, x = 'iteration', y = 'acc.test.mean', plot.type = 'line')
```

Benchmark

```{r}

lrns <- list(makeLearner(cl = 'classif.lda'), makeLearner('classif.rpart'))
rdesc <- makeResampleDesc(method = 'CV', iters = 10L)
bmr <- benchmark(learners = lrns, tasks = sonar.task, resamplings = rdesc, measures = mmce)

getBMRAggrPerformances(bmr, drop = TRUE, as.df = TRUE)
getBMRAggrPerformances(bmr)
getBMRPredictions(bmr)
getBMRPredictions(bmr, learner.ids = 'classif.rpart', as.df = TRUE)
getBMRTaskIds(bmr)
getBMRLearnerIds(bmr)
getBMRMeasureIds(bmr)
getBMRModels(bmr)
getBMRLearners(bmr)
getBMRMeasures(bmr)


lrns2 = list(makeLearner("classif.randomForest"), makeLearner("classif.qda"), makeLearner('classif.ksvm'))
bmr2 = benchmark(lrns2, sonar.task, rdesc, show.info = FALSE)
bmr2
mergeBenchmarkResults(list(bmr, bmr2))

rin <- getBMRPredictions(bmr)[[1]][[1]]$instance

bmr3 <- benchmark(lrns2, sonar.task, rin, show.info = FALSE)
bmr3
mergeBenchmarkResults(list(bmr, bmr3))


lrns = list(
  makeLearner("classif.lda", id = "lda"),
  makeLearner("classif.rpart", id = "rpart"),
  makeLearner("classif.randomForest", id = "randomForest")
)

ring.task = convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task = convertMLBenchObjToTask("mlbench.waveform", n = 600)

tasks = list(iris.task, sonar.task, pid.task, ring.task, wave.task)

rdesc = makeResampleDesc("CV", iters = 10)
meas = list(mmce, ber, timetrain)
bmr = benchmark(lrns, tasks, rdesc, meas, show.info = FALSE)

bmr
perf <- getBMRPerformances(bmr, as.df = TRUE)
plotBMRBoxplots(bmr = bmr, measure = mmce, order.lrns = getBMRLearnerIds(bmr = bmr))
plotBMRSummary(bmr = bmr)
m <- convertBMRToRankMatrix(bmr = bmr, measure = mmce)
plotBMRRanksAsBarChart(bmr, pos = 'tile', order.lrns = getBMRLearnerIds(bmr))

friedmanTestBMR(bmr)
friedmanPostHocTestBMR(bmr = bmr, p.value = 0.1)

perf <- getBMRPerformances(bmr, as.df = TRUE)

```

Parallelization
```{r}
library(parallelMap)
parallelMap::parallelStartMulticore(2)
rdesc = makeResampleDesc("CV", iters = 3)
r = resample("classif.lda", iris.task, rdesc)

parallelMap::parallelStop()
parallelGetRegisteredLevels()

```

Case study
```{r}
# 1. define learning task
# 2. tune the model
# 3. benchmark experiment
# 4. performance of the model

data(BostonHousing, package = 'mlbench')

regr.task <- makeRegrTask(data = BostonHousing, target = 'medv')
regr.task

set.seed(1234)
# define search space
ps_ksvm <- makeParamSet(
  makeNumericParam('sigma', lower = -12, upper = 12, trafo = function(x) 2^x)
)

ps_rf <- makeParamSet(
  makeIntegerParam('num.trees', lower = 1L, upper = 200L)
)

# choose resampling strategy
rdesc <- makeResampleDesc('CV', iters = 5L)

# choose performance measure
meas <- rmse

# choose tuning method
ctrl <- makeTuneControlCMAES(budget = 100L)

# tune wrappers
tuned.ksvm <- makeTuneWrapper(learner = 'regr.ksvm', resampling = rdesc, measures = meas, par.set = ps_ksvm, control = ctrl, show.info = FALSE)
tuned.rf <- makeTuneWrapper(learner = 'regr.ranger', resampling = rdesc, measures = meas, par.set = ps_rf, control = ctrl, show.info = FALSE)

# benchmark experiment
lrns <- list(makeLearner('regr.lm'), tuned.ksvm, tuned.rf)
bmr <- benchmark(learners = lrns, tasks = regr.task, resamplings = rdesc, measures = rmse, show.info = FALSE)
plotBMRBoxplots(bmr)

```

ROC analysis and performance curve

```{r}
n <- getTaskSize(sonar.task)
train.set <- sample(n, size = round(2/3 * n))
test.set <- setdiff(seq_len(n), train.set)

lrn1 <- makeLearner(cl = 'classif.lda', predict.type = 'prob')
mod1 <- train(learner = lrn1, task = sonar.task, subset = train.set)
pred1 <- predict(mod1, task = sonar.task, subset = test.set)

df <- generateThreshVsPerfData(obj = pred1, measures = list(fpr, tpr, mmce))
performance(pred1, auc)
plotROCCurves(df)
plotThreshVsPerf(obj = df)


lrn2 <- makeLearner(cl = 'classif.ksvm', predict.type = 'prob')
mod2 <- train(learner = lrn2, task = sonar.task, subset = train.set)
pred2 <- predict(mod2, task = sonar.task, subset = test.set)

df <- generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr))
plotROCCurves(df)
performance(pred2, auc)


df <- generateThreshVsPerfData(obj = list(lda = pred1, ksvm = pred2), measures = list(ppv, tpr))
plotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE)


```

```{r}
# Tune wrapper for ksvm
rdesc.inner <- makeResampleDesc('Holdout')
ms <- list(auc, mmce)

ps <- makeParamSet(
  makeDiscreteParam('C', 2^(-1:1))
)

ctrl <- makeTuneControlGrid()
lrn2 <- makeTuneWrapper(learner = lrn2, resampling = rdesc.inner, measures = ms, par.set = ps, control = ctrl, show.info = FALSE)

lrns <- list(lrn1, lrn2)

rdesc.outer <- makeResampleDesc('CV', iters = 5)
bmr <- benchmark(learners = lrns, tasks = sonar.task, resamplings = rdesc.outer, measures = ms, show.info = FALSE)
bmr
df <- generateThreshVsPerfData(obj = bmr, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
df <- generateThreshVsPerfData(obj = bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE)

plotROCCurves(df)

preds <- getBMRPredictions(bmr = bmr, drop = TRUE)

preds2 <- lapply(preds, function(x) {class(x) = "Prediction"; return(x)})
df = generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce))
plotROCCurves(df)


n = getTaskSize(sonar.task)
train.set = sample(n, size = round(2/3 * n))
test.set = setdiff(seq_len(n), train.set)

### Train and predict linear discriminant analysis
lrn1 = makeLearner("classif.lda", predict.type = "prob")
mod1 = train(lrn1, sonar.task, subset = train.set)
pred1 = predict(mod1, task = sonar.task, subset = test.set)

ROCRpred1 = asROCRPrediction(pred1)
ROCRperf1 <- ROCR::performance(ROCRpred1, 'tpr', 'fpr')
ROCR::plot(ROCRperf1)
ROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)

ch = ROCR::performance(ROCRpred1, "rch")

```














